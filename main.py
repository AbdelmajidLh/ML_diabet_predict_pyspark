# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TsFBIUuv5peP0tmgNrCABS1pBCVgqTUr

# PysPark ML Lib - Prédiction du diabète par régression logistique

> Réalisé par : Abdelmajid EL HOU - Consultant Data <br>
[ePortfolio](https://abdelmajidlh.github.io/ePortfolio/)  | [Github](https://github.com/AbdelmajidLh) | [Linkedin](https://www.linkedin.com/in/aelhou/)

## Contexte & Objectif

Le jeu de données provient du *National Institute of Diabetes and Digestive and Kidney Diseases*. L'objectif est de **prédire**, à partir de mesures diagnostiques, si un patient est diabétique. 

Le dataset est composé uniquement de femmes (> 21 ans) et est disponible sur Kaggle ([lien ici](https://www.kaggle.com/datasets/mathchi/diabetes-data-set)).

## Installation & importation des librairies
"""

# installation des librairies
#! pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.classification import LogisticRegressionModel
import pandas as ps

"""## lancer une session Spark (sparkSession)

Ce code crée une nouvelle session `Spark`.

La méthode `SparkSession.builder()` crée un nouveau builder pour construire une session `Spark`. L'appel à `appName("spark")` définit le nom de l'application Spark. Enfin, `getOrCreate()` crée une nouvelle session Spark si elle n'existe pas déjà, ou retourne la session existante si elle existe. La nouvelle session Spark est stockée dans la variable `spark`.
"""

spark = SparkSession.builder.appName("spark").getOrCreate()

"""## Importation & exploration des données

Le dataset est téléchargé à partir de github. Ces données viennent de Kaggle.
"""

#! git clone https://github.com/education454/diabetes_dataset
df = spark.read.csv('diabetes_dataset/diabetes.csv', header = True, inferSchema=True) # predit le type de colonnes
df.show(6)

"""la colonne `Outcome` est la variable de sortie. `0 : normal, 1 : diabétique`."""

# vérifier le type des colonnes dans le df
df.printSchema()

# Vérifier les dimensions du df
print(df.count(), ':', len(df.columns))

# Calculer le nombre de malades et de normaux dans le dataset
df.groupBy("Outcome").count().show()

"""* Nous avons 684 malades sur l'ensemble des 2000 individus."""

# faire un describe sur le df
df.describe().show()

"""Le tableau montre les statistiques basiques pour les colonnes numériques. La valeur minimale pour le glucose, l'insuline et la pression sanguine est égale à `0` !. Ces valeurs necessitent d'être nettoyées.

## Nettoyage des données - data cleaning
"""

# Vérifier les valeurs manquantes dans le df
for col in df.columns:
  print(col + ":", df[df[col].isNull()].count())

"""Notre *dataset* ne contiens aucune valeur manquante."""

# créer une fonction pour compter le nombre de valeurs 0 et leur pourcentage par colonne
def count_zeros(df, columns):
  for col in columns:
    num_zeros = df.filter(df[col] == 0).count()
    total_rows = df.count()
    percentage = (num_zeros / total_rows) * 100
    print("{} : {} ({:.2f}%)".format(col, num_zeros, percentage))

liste_cols = ['Glucose', 'Bloodpressure', 'SkinThickness', 'Insulin', 'BMI']
count_zeros(df, liste_cols)

"""Ces pourcentages indiquent que la colonne Glucose a le plus faible pourcentage de valeurs nulles, avec seulement 0,65%. La colonne Insulin a le pourcentage le plus élevé de valeurs nulles, avec 47,80%. Les autres colonnes ont des pourcentages de valeurs nulles compris entre 1,40% et 28,65%."""

# afficher la valeur moyenne pour chaque colonne et faire le remplacement
## méthode 1 :
for i in df.columns[1:6]:
  mean_val = df.agg({i:'mean'}).first()[0]
  print("la valeur moyenne de la colonne {} est : {}".format(i, int(mean_val)))
  # update the values : si la condition (val ==0) est vrai
  df = df.withColumn(i, when(df[i]==0, int(mean_val)).otherwise(df[i]))

df.show(10)

"""> Les valeurs sont bien remplacées.

## Construire et entrainer le modèle de machine learning
### Feature ingeneering
"""

# calculer la corrélation entre la variable de réponse et les autres variables
for col in df.columns:
  print('La correlation de  {} avec la variable outcome est {}.'.format(col, df.stat.corr('Outcome', col)))
  #print(f'La correlation de  {col} avec la variable outcome est {df.stat.corr('Outcome', col)}.')

"""Les résultats de corrélation indiquent que certains des facteurs peuvent avoir un impact significatif sur le résultat de la régression logistique. 

* Les variables `Glucose` et `BMI` ont les **plus fortes corrélations** avec le résultat, ce qui signifie qu'elles sont les plus susceptibles d'avoir un impact sur le résultat de la régression logistique. 
* Les variables `Pregnancies`, `BloodPressure`, `SkinThickness`, `Insulin` et `DiabetesPedigreeFunction` ont des **corrélations plus faibles** avec le résultat, ce qui signifie qu'elles sont moins susceptibles d'avoir un impact sur le résultat de la régression logistique. 
* La variable `Age` a une corrélation **modérée** avec le résultat, ce qui signifie qu'elle peut avoir un impact modéré sur le résultat de la régression logistique.
"""

# creer un vectorAssembler : c'est un feature transformer qui merge les différentes colonnes dans un seul vecteur (features).
inputCols = ['Pregnancies' , 'Glucose' , 'BloodPressure', 'SkinThickness' , 'Insulin' , 'BMI' , 'DiabetesPedigreeFunction' ,'Age'  ]
assembler = VectorAssembler(inputCols= inputCols, outputCol='features')
output_data = assembler.transform(df)

# vérifier si la colonne features est rajoutée au dataframe
output_data.printSchema()

# afficher les données (3 lignes)
output_data.show(3)

"""## Entrainer le modèle"""

# selectionner les colonnes d'interer
final_df = output_data.select('features', 'Outcome')
final_df.show(2)

# split to training (70%) and test (30%)
train, test = final_df.randomSplit([0.7, 0.3])

# créer le modèle
models = LogisticRegression(labelCol='Outcome')

# entrainer le modèle
model = models.fit(train)

# afficher le sommaire
summary = model.summary

## les prédictions
summary.predictions.show()
summary.predictions.describe().show()

"""## Evaluation du modèle

`BinaryClassificationEvaluator` de pysparkML est un outil d'évaluation qui permet de **mesurer la performance d'un modèle de classification binaire**. Il fournit des métriques telles que la `précision`, le `rappel (recall`), l'`aire sous la courbe ROC (AUC) : par défaut` et la `précision-rappel`. Ces métriques peuvent être utilisées pour comparer les **performances** des modèles et ainsi déterminer le meilleur modèle à utiliser.
"""

# feed test data in the model and evaluate it
predictions = model.evaluate(test)

# voir les prédictions
predictions.predictions.show(15)

"""Dans la plupart des cas (sur les 15 lignes affichées), le modèle de regression prédit bien le résultats."""

from pyspark.ml import evaluation
# évaluer le modèle
evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='Outcome')
evaluator.evaluate(model.transform(test))

"""Les résultats montrent que le modèle de régression a une **précision** de **83%**, ce qui est assez bon. Cela signifie que le modèle est capable de prédire avec une précision élevée si un individu aura un résultat positif (malade) ou négatif (diabétique).

## Sauvegarder le modèle

> On sauvegarde un modèle de machine learning afin de pouvoir le réutiliser à l'avenir. En effet, une fois que le modèle a été entraîné et qu'il fonctionne de manière optimale, il est important de le sauvegarder afin de pouvoir le réutiliser pour de nouvelles prédictions. Cela permettra d'économiser du temps et des ressources, car il n'est pas nécessaire de retrainer le modèle pour chaque nouvelle prédiction.
"""

model.save("LogReg_model")

"""## Réutiliser le modèle sauvegardé"""

model = LogisticRegressionModel.load('LogReg_model')

"""## Sources :

[School of Disruptive Innovation](https://www.udemy.com/course/data-science-hands-on-diabetes-prediction-with-pyspark-mllib/learn/lecture/20880872#overview)
"""